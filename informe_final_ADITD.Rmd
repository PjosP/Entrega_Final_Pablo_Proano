---
title: "Entrega_Final_PP"
author: "Pablo Proaño"
date: "2025-12-01"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **PROYECTO FINAL**
# Analítica de Datos Industriales para la Toma de Decisiones con Apoyo de AI (Curso ADI&TD-IA) 

## Datos del Estudiante:
**Nombre:** Pablo Proaño

**mail:** pablo.proano@epn.edu.ec

**Telf:** +593 98 244 0935

## Descripción del Proyecto

El presente proyecto tiene como objetivo analizar un conjunto de datos correspondiente a operaciones de importación registradas por el Servicio Nacional de Aduana del Ecuador (SENAE), específicamente del conjunto denominado SENAE_Importaciones Régimen General, el cual contiene el detalle de las importaciones por fecha de ingreso de la Declaración Aduanera de Importación (DAI) con estado de salida autorizada. La base de datos, disponible en formato CSV y actualizada a diciembre de 2025, es utilizada para realizar procesos de carga, limpieza, descripción estadística y visualización de variables relacionadas con valor, peso, cantidades e impuestos.

## Descripción de la Base  de Datos

La base de datos utilizada en este proyecto corresponde al conjunto SENAE_Importaciones Régimen General, publicado por el Servicio Nacional de Aduana del Ecuador (SENAE), y contiene información detallada de las operaciones de importación registradas mediante la Declaración Aduanera de Importación (DAI) con estado de salida autorizada. El conjunto de datos se presenta en formato CSV y está compuesto por variables de tipo categórico y numérico. 

## Objetivo del Proyecto

Para el desarrollo del proyecto se consideran como **variables de entrada** aquellas asociadas a la información conocida al arribo de la mercancía a Aduana, tales como SUBPARTIDA, PAIS_ORIGEN, REGIMEN, PESO_NETO, FOB, FLETE, SEGURO, CANTIDAD_FISICA y CANTIDAD_COMERCIAL. **Como variables de salida** se definen los principales tributos calculados por la autoridad aduanera, entre ellos ADVALOREM, ICE, FODINFA, SALVAGUARDIA e IVA. Si bien estos valores se determinan mediante ecuaciones, formulas , tablas arancelarias y normativa específica, el objetivo del proyecto es **desarrollar un modelo basado en datos** que permita estimar dichos tributos aun sin conocer explícitamente estas ecuaciones, utilizando únicamente la información histórica contenida en la base de datos. 

## Carga y tratamiento de Datos
En esta sección cargamos los datos de la base que conseguimos para este trabajo.
```{r}
library(readr)
aduana  <- read_delim("https://raw.githubusercontent.com/PjosP/Entrega_Final_Pablo_Proano/refs/heads/main/base_aduana_10.csv", 
    delim = "|", escape_double = FALSE, trim_ws = TRUE)
head(aduana)
```

Vamos a revisar las dimenciones de la base de datos:
```{r}
dim(aduana)
```
Vamos a ver de que tipo son los campos de la base de datos:
```{r}
sapply(aduana, class)
```

Vamos a revisar si existen elementos en blanco o elementos duplicados:
```{r}
anyNA(aduana)
anyDuplicated(aduana)
```
Podemos ver que no hay celdas en blanco ni duplicadas.

La base de datos tiene algunas columnas que no son relevantes para nuestro estudio, por lo que vamos a crear una nueva base solo con las columnas que por ahora pensamos que son relevantes y lo vamos a almacenar en una base de datos que se llame **base_seleccionada**:

```{r}
# Definimos las variables de entrada
vars_entrada <- c(
  "REGIMEN", "SUBPARTIDA", "PAIS_ORIGEN", "CONVENIO_INTERNACIONAL",
  "PESO_NETO", "CANTIDAD_FISICA", "CANTIDAD_COMERCIAL",
  "TIPO_UNIDAD_FISICA", "TIPO_UNIDAD_COMERCIAL",
  "FOB", "FLETE", "SEGURO", "CIF"
)

# Definimos las variables de salida
vars_salida <- c(
  "ADVALOREM","ICE_ADVALOREM",
  "FODINFA", "IVA"
)

# Unimos las variables seleccionadas
vars_seleccionadas <- c(vars_entrada, vars_salida)

# Creamos la nueva base de datos
base_seleccionada <- aduana[, vars_seleccionadas]

# Verificación básica
dim(base_seleccionada)
```

## Analisis multivariante

Para saber si nuestro proyecto es Viable, voy a obtener la matriz de acrianzaras, para analizar si las variables están relacionadas y podría predecir unas en función de otras.

Sin embargo, previo a esto voy a reemplazar las filas que tienen texto por números, y voy a almacenar los resultados en una variable llamada **base_numerica** 

```{r}
# Se identificaron automáticamente las columnas que son de tipo carácter (texto)
vars_char <- names(base_seleccionada)[sapply(base_seleccionada, is.character)]

# Se creó una copia de la base original para trabajar con la versión numérica
base_numerica <- base_seleccionada

# Se transformaron las variables categóricas a valores numéricos mediante codificación de factores
base_numerica[vars_char] <- lapply(base_numerica[vars_char], function(x) {
  as.numeric(as.factor(x))
})

# Se verificó el tipo de dato de todas las columnas luego de la transformación
sapply(base_numerica, class)

```
**Nota:** Yo solía hacer esto a mano, investigando en Internet, vi que había dos formas automáticas de hacerlo, la primera usando el método de factores y la segunda usando el método "One-Hot Encoding", la primera sirve para modelos basados en árboles y métodos de clasificación que no dependen de relaciones lineales, como voy a usar random forest en mi proyecto decidí usar esa para no hacer mas grande la base. 


Ahora voy a normalizar la base y la voy a guardar en una nueva base de datos llamada **base_normalizada**
```{r}
# Función de normalización Min-Max
normalizar_minmax <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

# Aplicación de la normalización a todas las columnas
base_normalizada <- as.data.frame(
  lapply(base_numerica, normalizar_minmax)
)

# Verificación
summary(base_normalizada)


```


Ahora voy a calcular la matriz de covarianzas a partir de la base de datos normalizada, con el fin de analizar el grado de variación conjunta entre las variables de entrada y salida, y evaluar su relación estadística previa a la construcción del modelo.

```{r}
# Se calculó la matriz de covarianzas de la base de datos normalizada
matriz_covarianzas <- cov(base_normalizada)

# Se visualiza la matriz de covarianzas
matriz_covarianzas

```

Esta matriz, me suele parecer un poco confusa, por lo que personalmente uso la matriz de correlaciones:

```{r}
# Se calculó la matriz de covarianzas de la base de datos normalizada
matriz_correlacion <- cor(base_normalizada)

# Se visualiza la matriz de covarianzas
matriz_correlacion
```



Voy a representar la matriz de correlaciones usando un mapa de calor, para analizar de forma visual la intensidad y el signo de la relación entre las variables de entrada y salida. Este gráfico facilitó la identificación de dependencias fuertes, débiles y relaciones inversas entre variables.
```{r}
library(corrplot)
# Se generó el gráfico de correlación
corrplot(matriz_correlacion, method = "color", type = "upper")
```

**Nota:** Tuve que instalar la librería "corrplot"

Para ver los resultados por separado, voy a generar gráficos de barras para cada una de las salidas, quiero ver cuales de las variables de entrada tienen mayor impacto en las salidas.

Esto lo hago por que no un gran numero de variables de entrada y no quiero usarlas todas, solo las 4 mas relevantes.

**Para el IVA:**
```{r}
# Se extrajo la correlación de IVA con todas las variables de entrada
cor_iva <- matriz_correlacion[vars_entrada, "IVA"]

# Se generó el gráfico de barras de las correlaciones
barplot(cor_iva,
        las = 2,                 # rota etiquetas del eje X
        main = "Correlación de IVA con las variables de entrada",
        ylab = "Coeficiente de correlación",
        xlab = "Variables de entrada")

```


**Para el FODINFA:**
```{r}
cor_FODINFA <- matriz_correlacion[vars_entrada, "FODINFA"]

# Se generó el gráfico de barras de las correlaciones
barplot(cor_FODINFA,
        las = 2,                 # rota etiquetas del eje X
        main = "Correlación de FODINFA con las variables de entrada",
        ylab = "Coeficiente de correlación",
        xlab = "Variables de entrada")

```


**Para el ADVALOREM:**
```{r}
cor_ADVALOREM <- matriz_correlacion[vars_entrada, "ADVALOREM"]

# Se generó el gráfico de barras de las correlaciones
barplot(cor_ADVALOREM,
        las = 2,                 # rota etiquetas del eje X
        main = "Correlación de ADVALOREM con las variables de entrada",
        ylab = "Coeficiente de correlación",
        xlab = "Variables de entrada")

```

**Para el ICE_ADVALOREM:**
```{r}
cor_ICE_ADVALOREM <- matriz_correlacion[vars_entrada, "ICE_ADVALOREM"]

# Se generó el gráfico de barras de las correlaciones
barplot(cor_ICE_ADVALOREM,
        las = 2,                 # rota etiquetas del eje X
        main = "Correlación de ICE_ADVALOREM con las variables de entrada",
        ylab = "Coeficiente de correlación",
        xlab = "Variables de entrada")

```

## Resultados

A partir del análisis de correlación se observa que, para la salida **IVA**, las variables de entrada con mayor impacto son:

- `FLETE`
- `FOB`
- `CIF`
- `TIPO_UNIDAD_COMERCIAL`

Para la salida **FODINFA**, las variables de entrada de mayor influencia son:

- `FLETE`
- `PESO_NETO`
- `CIF`
- `FOB`

Finalmente, para las salidas **ADVALOREM** e **ICE_ADVALOREM**, las variables de entrada más relevantes son:

- `FLETE`
- `FOB`
- `CIF`
- `REGIMEN`

De manera general, se puede concluir que las variables de entrada **`FLETE`, `FOB` y `CIF`** presentan el mayor impacto sobre todas las salidas analizadas, mientras que una cuarta variable (como `TIPO_UNIDAD_COMERCIAL`, `PESO_NETO` o `REGIMEN`) aporta información adicional específica dependiendo de cada tributo.

## Resultados del Análisis Multivariable

Como resultado del estudio, se analizó el impacto de las variables de entrada sobre las salidas del modelo, identificando aquellas con mayor relevancia para la predicción de los tributos. De esta forma, se pasó de considerar inicialmente **13 variables de entrada** a trabajar únicamente con **3 variables principales** (`FLETE`, `FOB` y `CIF`).

En la siguiente etapa se generarán **cuatro conjuntos de datos**, uno para cada variable de salida, en los cuales las tres primeras columnas corresponderán a las variables de entrada seleccionadas y la última columna a la salida específica de cada modelo.

```{r}
# Se crearon las bases de datos específicas para cada tributo
# usando como variables de entrada FLETE, FOB y CIF
# y como variable de salida el tributo correspondiente

base_IVA <- aduana[, c("FLETE", "FOB", "CIF","TIPO_UNIDAD_COMERCIAL", "IVA")]
base_ADVALOREM <- aduana[, c("FLETE", "FOB", "CIF", "ADVALOREM")]
base_ICE_ADVALOREM <- aduana[, c("FLETE", "FOB", "CIF", "ICE_ADVALOREM")]
base_FODINFA <- aduana[, c("FLETE", "FOB", "CIF", "FODINFA")]

# Verificación rápida de las cuatro bases creadas
list(
  IVA = dim(base_IVA),
  ADVALOREM = dim(base_ADVALOREM),
  ICE_ADVALOREM = dim(base_ICE_ADVALOREM),
  FODINFA = dim(base_FODINFA)
)


```
## Modelo Random Forest

Con el fin de modelar la relación no lineal entre las variables de entrada (`FLETE`, `FOB` y `CIF`) y los tributos aduaneros de salida (`IVA`, `ADVALOREM`, `ICE_ADVALOREM` y `FODINFA`), se emplea el algoritmo **Random Forest de regresión**. Este método basado en ensambles de árboles de decisión permite capturar interacciones complejas entre las variables, reducir la varianza del modelo y mejorar la capacidad de generalización, siendo especialmente adecuado para conjuntos de datos con relaciones no lineales y posible multicolinealidad entre las variables de entrada.

Primero cargamos la ibreria y nos aseguramos de que cada vez que se corra este programa se obtengan los mismos resultados:

```{r}
# Se cargó la librería necesaria para Random Forest
library(randomForest)
# Se fijó la semilla para garantizar reproducibilidad
set.seed(123)
```

Mi idea inicial fue utilizar el 75% de los datos para entrenar el modelo y el 25% restante para validarlo. Sin embargo, la base de datos original contiene **133 493** filas y, al aplicar el modelo de Random Forest sobre todo el conjunto, se presentaba un cuello de botella computacional que podía superar los 20 minutos de ejecución.

Por este motivo, se decidió trabajar con una **muestra aleatoria del 10%** de la base original, equivalente a **13 400 filas**, que se considera representativa para los fines de este trabajo. Sobre esta muestra se utiliza el **70% de los datos para el entrenamiento** de los modelos y el **30% restante para la validación**.


```{r}
n_total <- nrow(base_numerica)
tam_muestra <- min(13400, n_total)
idx_muestra <- sample(seq_len(n_total), size = tam_muestra)
aduana_muestra <- base_numerica[idx_muestra, ]
dim(aduana_muestra)
```

A partir de la muestra se construyeron las bases específicas para cada salida, utilizando como variables de entrada `FLETE`, `FOB` y `CIF`. Posteriormente, cada base se dividió en un conjunto de entrenamiento (70%) y otro de prueba (30%):

```{r}
#Bases específicas por salida, construidas a partir de la muestra
base_IVA <- aduana_muestra[, c("FLETE", "FOB", "CIF","TIPO_UNIDAD_COMERCIAL", "IVA")]
base_ADVALOREM <- aduana_muestra[, c("FLETE", "FOB", "CIF","REGIMEN", "ADVALOREM")]
base_ICE_ADVALOREM <- aduana_muestra[, c("FLETE", "FOB", "CIF","REGIMEN", "ICE_ADVALOREM")]
base_FODINFA <- aduana_muestra[, c("FLETE", "FOB", "CIF","PESO_NETO", "FODINFA")]

#División en entrenamiento (70%) y prueba (30%)

n_m <- nrow(aduana_muestra)
idx_entrenamiento <- sample(seq_len(n_m), size = floor(0.7 * n_m))

train_IVA <- base_IVA[idx_entrenamiento, ]
test_IVA <- base_IVA[-idx_entrenamiento, ]

train_ADVALOREM <- base_ADVALOREM[idx_entrenamiento, ]
test_ADVALOREM <- base_ADVALOREM[-idx_entrenamiento, ]

train_ICE_ADVALOREM <- base_ICE_ADVALOREM[idx_entrenamiento, ]
test_ICE_ADVALOREM <- base_ICE_ADVALOREM[-idx_entrenamiento, ]

train_FODINFA <- base_FODINFA[idx_entrenamiento, ]
test_FODINFA <- base_FODINFA[-idx_entrenamiento, ]

```

A continuación se ajustan los modelos de Random Forest de regresión para cada una de las salidas consideradas en el estudio:

```{r}
#Modelos Random Forest para cada salida

modelo_IVA <- randomForest(
IVA ~ FLETE + FOB + CIF + TIPO_UNIDAD_COMERCIAL,
data = train_IVA,
ntree = 150,
mtry = 2,
importance = TRUE
)

modelo_ADVALOREM <- randomForest(
ADVALOREM ~ FLETE + FOB + CIF +REGIMEN,
data = train_ADVALOREM,
ntree = 150,
mtry = 2,
importance = TRUE
)

modelo_ICE_ADVALOREM <- randomForest(
ICE_ADVALOREM ~ FLETE + FOB + CIF +REGIMEN,
data = train_ICE_ADVALOREM,
ntree = 150,
mtry = 2,
importance = TRUE
)

modelo_FODINFA <- randomForest(
FODINFA ~ FLETE + FOB + CIF + PESO_NETO,
data = train_FODINFA,
ntree = 150,
mtry = 2,
importance = TRUE
)
```

Con los modelos ajustados, se generan las predicciones sobre el conjunto de prueba y se calculan los indicadores de desempeño RMSE y \(R^2\) para cada salida:

```{r}
# Predicciones sobre el conjunto de prueba

pred_IVA           <- predict(modelo_IVA, test_IVA)
pred_ADVALOREM     <- predict(modelo_ADVALOREM, test_ADVALOREM)
pred_ICE_ADVALOREM <- predict(modelo_ICE_ADVALOREM, test_ICE_ADVALOREM)
pred_FODINFA       <- predict(modelo_FODINFA, test_FODINFA)

# Funciones para RMSE y R²

rmse <- function(real, pred) {
sqrt(mean((real - pred)^2))
}

r2 <- function(real, pred) {
1 - sum((real - pred)^2) / sum((real - mean(real))^2)
}

# Cálculo de métricas para cada modelo

resultados_modelos <- data.frame(
Salida = c("IVA", "ADVALOREM", "ICE_ADVALOREM", "FODINFA"),
RMSE = c(
rmse(test_IVA$IVA, pred_IVA),
rmse(test_ADVALOREM$ADVALOREM, pred_ADVALOREM),
rmse(test_ICE_ADVALOREM$ICE_ADVALOREM, pred_ICE_ADVALOREM),
rmse(test_FODINFA$FODINFA, pred_FODINFA)
),
R2 = c(
r2(test_IVA$IVA, pred_IVA),
r2(test_ADVALOREM$ADVALOREM, pred_ADVALOREM),
r2(test_ICE_ADVALOREM$ICE_ADVALOREM, pred_ICE_ADVALOREM),
r2(test_FODINFA$FODINFA, pred_FODINFA)
)
)

resultados_modelos


```
Los resultados obtenidos con el modelo Random Forest muestran un comportamiento diferenciado entre las variables de salida analizadas. Para el **IVA**, el coeficiente de determinación alcanzó un valor de \(R^2 = 0.5196\), lo que indica una capacidad predictiva moderada al incorporar las modificaciones realizadas al conjunto de datos. En el caso de **ADVALOREM** e **ICE\_ADVALOREM**, los valores negativos de \(R^2\) confirman que el modelo no logra capturar adecuadamente la variabilidad de estos tributos, dado que su cálculo depende principalmente de factores normativos y arancelarios. Por su parte, **FODINFA** presenta nuevamente un ajuste sobresaliente con un \(R^2 = 0.9856\), evidenciando una relación altamente determinística con las variables de entrada consideradas.

- **IVA**:
  - Se obtuvo un \(R^2 = 0.5196\), lo que indica que el modelo explica aproximadamente el 52% de la variabilidad del impuesto.
  - El valor de RMSE = 9393.92 refleja un error de predicción moderado en la escala original del tributo.
  - Este resultado confirma que el IVA puede ser estimado parcialmente a partir de variables monetarias y logísticas, aunque sigue dependiendo de factores adicionales de carácter normativo.

- **ADVALOREM**:
  - Se obtuvo un \(R^2 = -0.4860\), lo que indica que el modelo tiene un desempeño peor que el uso del valor promedio como estimador.
  - El RMSE = 9909.79 evidencia errores elevados en la predicción.
  - Esto confirma que el ADVALOREM está dominado por reglas arancelarias asociadas a la subpartida, el régimen y los convenios, las cuales no pueden ser capturadas adecuadamente por el modelo bajo este enfoque.

- **ICE\_ADVALOREM**:
  - El \(R^2 = -0.2783\) muestra que el modelo no logra identificar un patrón estadístico útil para esta variable.
  - El RMSE = 3627.67 sigue siendo elevado en relación con la magnitud del impuesto.
  - Este comportamiento confirma que el ICE depende de criterios específicos de tipo de mercancía y normativa tributaria, más que de relaciones continuas entre variables monetarias.

- **FODINFA**:
  - Se alcanzó un \(R^2 = 0.9856\), lo que representa un ajuste excelente entre los valores reales y los predichos.
  - El RMSE = 71.78 es bajo en relación con la magnitud típica del tributo.
  - Este resultado confirma que el FODINFA mantiene una relac

